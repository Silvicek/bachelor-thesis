\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Used methods}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Overview of the Thesis}{chapter.1}% 3
\BOOKMARK [0][-]{chapter.2}{Robot, Simulator}{}% 4
\BOOKMARK [1][-]{section.2.1}{Simulator Selection}{chapter.2}% 5
\BOOKMARK [1][-]{section.2.2}{Simulation run structure}{chapter.2}% 6
\BOOKMARK [2][-]{subsection.2.2.1}{Simulation details}{section.2.2}% 7
\BOOKMARK [1][-]{section.2.3}{Robot}{chapter.2}% 8
\BOOKMARK [2][-]{subsection.2.3.1}{State-Action space}{section.2.3}% 9
\BOOKMARK [0][-]{chapter.3}{Supervised Learning}{}% 10
\BOOKMARK [1][-]{section.3.1}{Introduction}{chapter.3}% 11
\BOOKMARK [1][-]{section.3.2}{Artificial Neural Networks}{chapter.3}% 12
\BOOKMARK [1][-]{section.3.3}{Recurrent Neural Networks}{chapter.3}% 13
\BOOKMARK [2][-]{subsection.3.3.1}{Motivation}{section.3.3}% 14
\BOOKMARK [2][-]{subsection.3.3.2}{Theory}{section.3.3}% 15
\BOOKMARK [2][-]{subsection.3.3.3}{Fully recurrent network}{section.3.3}% 16
\BOOKMARK [2][-]{subsection.3.3.4}{Long short term memory}{section.3.3}% 17
\BOOKMARK [1][-]{section.3.4}{Optimization}{chapter.3}% 18
\BOOKMARK [2][-]{subsection.3.4.1}{Stochastic gradient descent}{section.3.4}% 19
\BOOKMARK [2][-]{subsection.3.4.2}{Adam}{section.3.4}% 20
\BOOKMARK [2][-]{subsection.3.4.3}{Overfitting, Regularization}{section.3.4}% 21
\BOOKMARK [1][-]{section.3.5}{Experiments}{chapter.3}% 22
\BOOKMARK [2][-]{subsection.3.5.1}{Data}{section.3.5}% 23
\BOOKMARK [2][-]{subsection.3.5.2}{Implementation, setup}{section.3.5}% 24
\BOOKMARK [2][-]{subsection.3.5.3}{Network notation}{section.3.5}% 25
\BOOKMARK [2][-]{subsection.3.5.4}{Experiments}{section.3.5}% 26
\BOOKMARK [1][-]{section.3.6}{Conclusion}{chapter.3}% 27
\BOOKMARK [0][-]{chapter.4}{Reinforcement Learning}{}% 28
\BOOKMARK [1][-]{section.4.1}{Introduction}{chapter.4}% 29
\BOOKMARK [2][-]{subsection.4.1.1}{Policy}{section.4.1}% 30
\BOOKMARK [2][-]{subsection.4.1.2}{Reward}{section.4.1}% 31
\BOOKMARK [2][-]{subsection.4.1.3}{Finite differences}{section.4.1}% 32
\BOOKMARK [2][-]{subsection.4.1.4}{Action-Value function}{section.4.1}% 33
\BOOKMARK [1][-]{section.4.2}{Action-Value function approximation}{chapter.4}% 34
\BOOKMARK [2][-]{subsection.4.2.1}{Q-learning}{section.4.2}% 35
\BOOKMARK [1][-]{section.4.3}{Deep Q-learning}{chapter.4}% 36
\BOOKMARK [1][-]{section.4.4}{Actor-Critic Algorithms}{chapter.4}% 37
\BOOKMARK [2][-]{subsection.4.4.1}{Deterministic Policy Gradient}{section.4.4}% 38
\BOOKMARK [0][-]{chapter.5}{Deep Deterministic Policy Gradient}{}% 39
\BOOKMARK [1][-]{section.5.1}{The DDPG Algorithm}{chapter.5}% 40
\BOOKMARK [1][-]{section.5.2}{Implementation}{chapter.5}% 41
\BOOKMARK [2][-]{subsection.5.2.1}{DPG Simplification}{section.5.2}% 42
\BOOKMARK [2][-]{subsection.5.2.2}{Critic updates}{section.5.2}% 43
\BOOKMARK [2][-]{subsection.5.2.3}{Adam}{section.5.2}% 44
\BOOKMARK [1][-]{section.5.3}{Exploration vs Exploitation}{chapter.5}% 45
\BOOKMARK [2][-]{subsection.5.3.1}{Ornstein-Uhlenbeck Process}{section.5.3}% 46
\BOOKMARK [1][-]{section.5.4}{Prioritized Experience Replay}{chapter.5}% 47
\BOOKMARK [2][-]{subsection.5.4.1}{Implementation}{section.5.4}% 48
\BOOKMARK [1][-]{section.5.5}{Experiments}{chapter.5}% 49
\BOOKMARK [2][-]{subsection.5.5.1}{Cartpole simulation}{section.5.5}% 50
\BOOKMARK [2][-]{subsection.5.5.2}{Robot}{section.5.5}% 51
\BOOKMARK [0][-]{chapter.6}{Conclusion}{}% 52
\BOOKMARK [1][-]{section.6.1}{Future Work}{chapter.6}% 53
\BOOKMARK [0][-]{appendix.A}{Training Details}{}% 54
\BOOKMARK [1][-]{section.A.1}{Supervised Learning}{appendix.A}% 55
\BOOKMARK [1][-]{section.A.2}{DDPG}{appendix.A}% 56
\BOOKMARK [0][-]{appendix.B}{CD structure}{}% 57
\BOOKMARK [0][-]{appendix.C}{Bibliography}{}% 58
\BOOKMARK [0][-]{section*.12}{Project Specification}{}% 59
